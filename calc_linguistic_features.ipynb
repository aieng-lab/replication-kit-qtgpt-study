{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "330a454d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import spacy\n",
    "from lexicalrichness import LexicalRichness\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "answers = pd.read_csv(\"data/generated4_gpt4b_clean_octo_pair_id.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "38f6c617",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocesses data with spaCy for later use\n",
    "\n",
    "answers[\"question_spacy\"] = answers[\"question\"].apply(lambda x: nlp(x))\n",
    "\n",
    "answers[\"REAL_spacy\"] = answers[\"real_answer\"].apply(lambda x: nlp(x))\n",
    "answers[\"REAL_lemma\"] = answers[\"REAL_spacy\"].apply(lambda x: \" \".join([y.lemma_ for y in x]))\n",
    "\n",
    "answers[\"GPT_spacy\"] = answers[\"generated_answer\"].apply(lambda x: nlp(x))\n",
    "answers[\"GPT_lemma\"] = answers[\"GPT_spacy\"].apply(lambda x: \" \".join([y.lemma_ for y in x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bad396ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Countes the number of sentences per essay\n",
    "\n",
    "def num_of_sent(text):\n",
    "    return len([sentence for sentence in text.sents])\n",
    "\n",
    "answers[\"REAL_sent_count\"] = answers[\"REAL_spacy\"].apply(lambda x: num_of_sent(x))\n",
    "answers[\"GPT_sent_count\"] = answers[\"GPT_spacy\"].apply(lambda x: num_of_sent(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "34021711",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Countes the number of words per essay\n",
    "\n",
    "def num_of_words(text):\n",
    "    count = len([token.text.lower() for token in text if not token.is_stop and not token.is_punct and not token.is_space])\n",
    "    return count\n",
    "\n",
    "answers[\"question_word_count\"] = answers[\"question_spacy\"].apply(lambda x: num_of_words(x))\n",
    "answers[\"REAL_word_count\"] = answers[\"REAL_spacy\"].apply(lambda x: num_of_words(x))\n",
    "answers[\"GPT_word_count\"] = answers[\"GPT_spacy\"].apply(lambda x: num_of_words(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9c9782",
   "metadata": {},
   "source": [
    "### Sentence complexity\n",
    "\n",
    "Based on a number of particular dependnecy labels found in each sentence (Clausal modifier of noun; Conjunct; Adverbial clause modifier; Clausal complement; Clausal subject; Discourse; Parataxis)\n",
    "The output values are mean values of the number of the tags per sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "54c97b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcualtes the number of specified dependency label within a sentence\n",
    "def calculate_dep_score(text):\n",
    "    temp = []\n",
    "    for sentence in nlp(text).sents:\n",
    "        temp.append(sent_complexity_structure(sentence))\n",
    "    return np.mean(temp)\n",
    "\n",
    "# Return the number of specified dependency labels found\n",
    "def sent_complexity_structure(doc):\n",
    "    return len([token for token in doc if (token.dep_ == \"acl\" or token.dep_ == \"conj\" or token.dep_ == \"advcl\"or token.dep_ == \"ccomp\"\n",
    "                                            or token.dep_ == \"csubj\" or token.dep_ == \"discourse\" or token.dep_ == \"parataxis\")])\n",
    "\n",
    "# TODO: fix col name\n",
    "answers[\"REAL_sent_complex_tags\"] = answers[\"real_answer\"].apply(lambda x: calculate_dep_score(x))\n",
    "answers[\"GPT_sent_complex_tags\"] = answers[\"generated_answer\"].apply(lambda x: calculate_dep_score(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d40987c",
   "metadata": {},
   "source": [
    "### Lexical diversity\n",
    "\n",
    "Calculating lexical diverstity score using MTLD measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "755aa017",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculates MTLD score for the whole essay\n",
    "\n",
    "def calculate_lex_richness_MTLD2(text):\n",
    "    lex = LexicalRichness(text) \n",
    "    lex_rich_score = lex.mtld()\n",
    "    return(lex_rich_score)\n",
    "\n",
    "answers[\"REAL_LD\"] = answers[\"real_answer\"].apply(lambda x: calculate_lex_richness_MTLD2(x))\n",
    "answers[\"GPT_LD\"] = answers[\"generated_answer\"].apply(lambda x: calculate_lex_richness_MTLD2(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b4e2f7",
   "metadata": {},
   "source": [
    "### Discourse markers\n",
    "\n",
    "Calculating number of discourse markers from Penn Discourse Tree Bank per essay. Some discourse markers (about, as, by, both, for, from, given, in, like, on, once, only, still, when, with, without, yet, and) were excluded from the list because they can often be used as not discourse markers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dd52af63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counts the number of discourse markers using PDTB list\n",
    "\n",
    "discourse = pd.read_csv(\"markers/connectives_discourse_markers_PDTB.txt\", sep=\"\\'\", encoding=\"UTF-8\", header=None, usecols = [1,3])\n",
    "\n",
    "discourse[3] = discourse[3].apply(lambda x: x.replace(\"t_conn_\", \"\"))\n",
    "discourse[1] = discourse[1].apply(lambda x: \" \" + x + \" \")\n",
    "discourse.sort_values(3, inplace=True, ascending=False)\n",
    "\n",
    "# Countes the total numbers of discourse markers per essay\n",
    "def count_discourse_markers(text):\n",
    "    i = 0\n",
    "    for marker in discourse.itertuples():\n",
    "        if marker[1] in text:\n",
    "            i += text.count(marker[1])\n",
    "    return i\n",
    "\n",
    "\n",
    "answers[\"REAL_discourse\"] = answers[\"REAL_lemma\"].apply(lambda x: count_discourse_markers(x))\n",
    "answers[\"GPT_discourse\"] = answers[\"GPT_lemma\"].apply(lambda x: count_discourse_markers(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9cfa5d",
   "metadata": {},
   "source": [
    "### Modals\n",
    "\n",
    "Counting the number of modals using POS tag \"MD\" and the modals.csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "064aa75a",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Counts the number of modals from the list of modals\n",
    "\n",
    "modals = pd.read_csv(\"markers/modals.csv\", sep=\",\", encoding=\"UTF-8\", header=None)\n",
    "modals[0] = modals[0].apply(lambda x: x.replace('_', ' '))\n",
    "\n",
    "# Counts the number of modals per essay\n",
    "def count_total_modals(text):\n",
    "    counter = 0\n",
    "    for modal in modals.itertuples():\n",
    "        if modal[1] in text:\n",
    "            counter += text.count(modal[1])\n",
    "    return counter\n",
    "\n",
    "answers[\"REAL_modals1\"] = answers[\"REAL_lemma\"].apply(lambda x: count_total_modals(x))\n",
    "answers[\"GPT_modals1\"] = answers[\"GPT_lemma\"].apply(lambda x: count_total_modals(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1814edca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counts the number of modals using POS tagging\n",
    "\n",
    "answers[\"REAL_pos\"] = answers[\"REAL_spacy\"].apply(lambda x: \" \".join([y.tag_ for y in x]))\n",
    "answers[\"GPT_pos\"] = answers[\"GPT_spacy\"].apply(lambda x: \" \".join([y.tag_ for y in x]))\n",
    "\n",
    "answers[\"REAL_modals2\"] = answers[\"REAL_pos\"].str.count(r'MD')\n",
    "answers[\"GPT_modals2\"] = answers[\"GPT_pos\"].str.count(r'MD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1c1132c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculates total number of modals per essay\n",
    "\n",
    "answers[\"REAL_modals_all\"] = answers[\"REAL_modals2\"] + answers[\"REAL_modals1\"]\n",
    "answers[\"GPT_modals_all\"] = answers[\"GPT_modals2\"] + answers[\"GPT_modals1\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ba67d1",
   "metadata": {},
   "source": [
    "### Epistemic markers\n",
    " \n",
    "Getting the number of epistemic markers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2758a91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counts the total number of epistemic markers per essay\n",
    "\n",
    "def find_epistemic_markers(text):\n",
    "    epistemic_markers_count = []\n",
    "    epistemic_markers_count.extend(re.findall(r'(?:I|We|we|One|one)(?:\\s\\w+)?(?:\\s\\w+)?\\s(?:believes?|think|thinks|means?|worry|worries|know|guesse?s?|assumes?|wonders?|feels?)\\b(?:that)?', text))\n",
    "    epistemic_markers_count.extend(re.findall(r'(?:I|We|we|One|one)\\s(?:don\\'t|\\sdoesn\\'t\\sdo\\snot|\\sdoes\\snot)\\s(?:believe|think|mean|worry|know|guess|assume|wonder|feel)\\b(?:that)?', text))\n",
    "    epistemic_markers_count.extend(re.findall(r'(?:It|it)\\sis\\s(?:believed|known|assumed|thought)\\b(?:that)?', text))\n",
    "    epistemic_markers_count.extend(re.findall(r'(?:I|We|we)\\s(?:am|are|was|were)(?:\\sjust)?\\s(?:thinking|guessing|wondering)\\b(?:that)?', text))\n",
    "    epistemic_markers_count.extend(re.findall(r'(?:I\\'m|[Ww]e\\'re)(?:\\sjust)?\\s(?:thinking|guessing|wondering)\\b(?:that)?', text)) \n",
    "    epistemic_markers_count.extend(re.findall(r'(?:I|We|we|One|one)(?:\\s\\w+)?\\s(?:do|does)\\snot\\s(?:believe?|think|know)\\b(?:that)?', text)) \n",
    "    epistemic_markers_count.extend(re.findall(r'(?:I|We|we|One|one)\\swould(?:\\s\\w+)?(?:\\snot)?\\ssay\\b(?:that)?', text)) \n",
    "    epistemic_markers_count.extend(re.findall(r'(?:I\\sam|I\\'m)(?:\\s\\w+)?\\s(?:afraid|sure|confident)\\b(?:that)?', text)) \n",
    "    epistemic_markers_count.extend(re.findall(r'(?:My|my|Our|our)\\s(?:personal\\s)?(?:experience|opinion|belief|view|knowledge|worry|worries|concerns?|guesse?s?|position|perception)(?:\\son\\s\\w+)?\\s(?:is|are)\\b(?:that)?', text)) \n",
    "    epistemic_markers_count.extend(re.findall(r'[Ii]n\\s(?:my|our)(?:\\s\\w+)?\\s(?:view|opinion)\\b',text))\n",
    "    epistemic_markers_count.extend(re.findall(r'[Fr]rom\\s(?:my|our)\\s(?:point\\sof\\sview|perspective)\\b', text)) \n",
    "    epistemic_markers_count.extend(re.findall(r'As\\sfar\\sas\\s(?:I|We|we)\\s(?:am|are)\\sconcerned', text))\n",
    "    epistemic_markers_count.extend(re.findall(r'(?:I|We|we|One|one)\\s(?:can|could|may|might)(?:\\s\\w+)?\\sconclude\\b(?:that)?', text)) \n",
    "    epistemic_markers_count.extend(re.findall(r'I\\s(?:am\\swilling\\sto|must)\\ssay\\b(?:that)?', text)) \n",
    "    epistemic_markers_count.extend(re.findall(r'\"One\\s(?:can|could|may|might)\\ssay\\b(?:that)?', text)) \n",
    "    epistemic_markers_count.extend(re.findall(r'[Oo]ne\\s(?:can|could|may|might)\\ssay\\b(?:that)?',text)) \n",
    "    epistemic_markers_count.extend(re.findall(r'[Ii]t\\sis\\s(?:obvious|(?:un)?clear)\\b', text)) \n",
    "    epistemic_markers_count.extend(re.findall(r'[Ii]t(?:\\sjust)?\\s(?:seems|feels|looks)', text)) \n",
    "    epistemic_markers_count.extend(re.findall(r'[Pp]ersonally\\s(?:for\\sme|speaking)', text))\n",
    "    epistemic_markers_count.extend(re.findall(r'(?:[Ff]rankly|[Hh]onestly|[Cc]learly)', text))\n",
    "    return len(epistemic_markers_count)\n",
    "\n",
    "answers[\"REAL_EpMarkers\"] = answers[\"real_answer\"].apply(lambda x: find_epistemic_markers(x))\n",
    "answers[\"GPT_EpMarkers\"] = answers[\"generated_answer\"].apply(lambda x: find_epistemic_markers(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b60af5d",
   "metadata": {},
   "source": [
    "### Nominalisations\n",
    "\n",
    "Counting the number of nominalisations per essay. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "12205383",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counts the total number of nominalisations per essay\n",
    "\n",
    "def nominalisation_counter(text):\n",
    "    suffixes_n = r'\\b[A-Z]*\\w+(?:tion|ment|ance|ence|ion|it(?:y|ies)|ness|ship)(?:s|es)?\\b'\n",
    "    \n",
    "    nom_nouns = []    \n",
    "    nouns = [token.text for token in text if token.pos_ == 'NOUN']  \n",
    "    nom_nouns = [noun for noun in nouns if re.match(suffixes_n, noun)] \n",
    "    \n",
    "    return(len(nom_nouns))\n",
    "    \n",
    "answers[\"REAL_nominalisation\"] = answers[\"REAL_spacy\"].apply(lambda x: nominalisation_counter(x))\n",
    "answers[\"GPT_nominalisation\"] = answers[\"GPT_spacy\"].apply(lambda x: nominalisation_counter(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586a3c5a",
   "metadata": {},
   "source": [
    "### Compute question/answer word overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3480ceb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_overlap(text1, text2):\n",
    "    text1_tokens = set([token.text.lower() for token in text1 if not token.is_stop and not token.is_punct and not token.is_space])\n",
    "    text2_tokens = set([token.text.lower() for token in text2 if not token.is_stop and not token.is_punct and not token.is_space])\n",
    "    return len(text1_tokens.intersection(text2_tokens))\n",
    "\n",
    "answers[\"REAL_overlap\"] = answers.apply(lambda x: compute_overlap(x[\"question_spacy\"], x[\"REAL_spacy\"]), axis=1)\n",
    "answers[\"GPT_overlap\"] = answers.apply(lambda x: compute_overlap(x[\"question_spacy\"], x[\"GPT_spacy\"]), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba99a62",
   "metadata": {},
   "source": [
    "### Compute averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5935b64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counts the average number of features (discourse markers, modals, epistemic markers, nominalisations) per sentence for each essay\n",
    "\n",
    "def average_per_sentence(feature, sent):\n",
    "    average = feature/sent\n",
    "    return(average)\n",
    "\n",
    "answers[\"REAL_dm_per_sent\"] = answers.apply(lambda row: average_per_sentence(row[\"REAL_discourse\"], row[\"REAL_sent_count\"]), axis=1)\n",
    "answers[\"GPT_dm_per_sent\"] = answers.apply(lambda row: average_per_sentence(row[\"GPT_discourse\"], row[\"GPT_sent_count\"]), axis=1)\n",
    "\n",
    "answers[\"REAL_mod_per_sent\"] = answers.apply(lambda row: average_per_sentence(row[\"REAL_modals_all\"], row[\"REAL_sent_count\"]), axis=1)\n",
    "answers[\"GPT_mod_per_sent\"] = answers.apply(lambda row: average_per_sentence(row[\"GPT_modals_all\"], row[\"GPT_sent_count\"]), axis=1)\n",
    "\n",
    "answers[\"REAL_ep_per_sent\"] = answers.apply(lambda row: average_per_sentence(row[\"REAL_EpMarkers\"], row[\"REAL_sent_count\"]), axis=1)\n",
    "answers[\"GPT_ep_per_sent\"] = answers.apply(lambda row: average_per_sentence(row[\"GPT_EpMarkers\"], row[\"GPT_sent_count\"]), axis=1)\n",
    "\n",
    "answers[\"REAL_nom_per_sent\"] = answers.apply(lambda row: average_per_sentence(row[\"REAL_nominalisation\"], row[\"REAL_sent_count\"]), axis=1)\n",
    "answers[\"GPT_nom_per_sent\"] = answers.apply(lambda row: average_per_sentence(row[\"GPT_nominalisation\"], row[\"GPT_sent_count\"]), axis=1)\n",
    "\n",
    "answers[\"REAL_overlap_per_word\"] = answers.apply(lambda row: average_per_sentence(row[\"REAL_overlap\"], row[\"question_word_count\"]), axis=1)\n",
    "answers[\"GPT_overlap_per_word\"] = answers.apply(lambda row: average_per_sentence(row[\"GPT_overlap\"], row[\"question_word_count\"]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "240f35f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sentence complexity\n",
      "Real: 2.4036480035927568\n",
      "GPT:  2.147702687269995\n",
      "\n",
      "Average MTLD lexical diversity score\n",
      "Real:  68.50903895647262\n",
      "GPT: 104.84397511886124\n",
      "\n",
      "Average number of discourse markers per answer\n",
      "Real: 0.7106143722156866\n",
      "GPT:  0.5489400129784745\n",
      "\n",
      "Average number of modals per answer\n",
      "Real: 0.6309684307031302\n",
      "GPT:  0.5679658215715908\n",
      "\n",
      "Average number of epistemic markers per answer\n",
      "Real: 0.266035858117717\n",
      "GPT:  0.04532062488793258\n",
      "\n",
      "Average number of nominalisations per answer\n",
      "Real: 0.6482649640275412\n",
      "GPT:  1.3725764406533638\n",
      "\n",
      "Average overlap with question per answer normalized by question length\n",
      "Real: 0.20403642604922023\n",
      "GPT:  0.5591483720471176\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Average sentence complexity\")\n",
    "print(\"Real:\", np.mean(answers[\"REAL_sent_complex_tags\"]))\n",
    "print(\"GPT: \", np.mean(answers[\"GPT_sent_complex_tags\"]))\n",
    "print()\n",
    "\n",
    "print(\"Average MTLD lexical diversity score\")\n",
    "print(\"Real: \", np.mean(answers[\"REAL_LD\"]))\n",
    "print(\"GPT:\", np.mean(answers[\"GPT_LD\"]))\n",
    "print()\n",
    "\n",
    "print(\"Average number of discourse markers per answer\")\n",
    "print(\"Real:\", np.mean(answers[\"REAL_dm_per_sent\"]))\n",
    "print(\"GPT: \", np.mean(answers[\"GPT_dm_per_sent\"]))\n",
    "print()\n",
    "\n",
    "print(\"Average number of modals per answer\")\n",
    "print(\"Real:\", np.mean(answers[\"REAL_mod_per_sent\"]))\n",
    "print(\"GPT: \", np.mean(answers[\"GPT_mod_per_sent\"]))\n",
    "print()\n",
    "\n",
    "print(\"Average number of epistemic markers per answer\")\n",
    "print(\"Real:\", np.mean(answers[\"REAL_ep_per_sent\"]))\n",
    "print(\"GPT: \", np.mean(answers[\"GPT_ep_per_sent\"]))\n",
    "print()\n",
    "\n",
    "print(\"Average number of nominalisations per answer\")\n",
    "print(\"Real:\", np.mean(answers[\"REAL_nom_per_sent\"]))\n",
    "print(\"GPT: \", np.mean(answers[\"GPT_nom_per_sent\"]))\n",
    "print()\n",
    "\n",
    "print(\"Average overlap with question per answer normalized by question length\")\n",
    "print(\"Real:\", np.mean(answers[\"REAL_overlap_per_word\"]))\n",
    "print(\"GPT: \", np.mean(answers[\"GPT_overlap_per_word\"]))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cb4ca0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "answers.to_csv(\"data/answers-with-linguistic-markers.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "657977a8e1cca15745b845f6b06b6c4354282a12f318ae778563a98fa73562e6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
